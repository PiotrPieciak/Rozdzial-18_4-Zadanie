{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fbcc788-4cfa-49a1-a4c5-ac79104d6186",
   "metadata": {},
   "source": [
    "# 18.4 Zadanie Fashion-Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ce7b7-4d4e-4bac-acab-79446c9bf48a",
   "metadata": {},
   "source": [
    "#### import niezbednych bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5764a78-e959-4771-891d-758fcf57762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939e445c-6f94-4cc5-b103-26ba1a647b75",
   "metadata": {},
   "source": [
    "#### dane które są zaimportowane są już rozdzielone na testowe i treningowe, by móc zastosować train_test_split(images, labels, test_size=0.1, random_state=10, stratify=labels) połączymy oba zestawy danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c4cd71-63f7-4b4b-8ffc-c4bc71e79c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
    "images_train, labels_train = train\n",
    "images_test, labels_test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd63ed-799e-492c-8e51-57b60aae06da",
   "metadata": {},
   "source": [
    "#### Skoro mamy wszytskie dane pod zmienną images, można zastosowac normalizację obrazów oraz stworzyć dodatkowy wymiar niezbedny do szkolenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47954e7b-f392-49de-a1ec-e7de1b9591ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.concatenate((images_train, images_test), axis=0)\n",
    "images = images/255.0\n",
    "images = images[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "250b9e94-6aed-478e-af62-ebaffd09a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "labels =  np.concatenate((labels_train, labels_test), axis=0)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56741b5f-ef79-466e-9afa-44092499847d",
   "metadata": {},
   "source": [
    "#### Rozdzielamy dane zgodnie z tereścią zadania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90198a10-15e1-4161-a313-634d176b1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.1, random_state=10, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aabe41-2df3-4b55-b92b-1f7839eacad5",
   "metadata": {},
   "source": [
    "#### Tworzymy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7dafe22-c6d2-4075-b877-a3a5635c6576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piotr\\anaconda3\\envs\\condaenv4\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),  \n",
    "    tf.keras.layers.Dense(128, activation='relu'),     \n",
    "    tf.keras.layers.Dense(64, activation='relu'),     \n",
    "    tf.keras.layers.Dense(10, activation='softmax')   \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9930c-196e-4992-be07-a217b82a6a97",
   "metadata": {},
   "source": [
    "#### Kompilujemy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b990c79-4a47-4b32-8537-127e2c09f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7a154-4d82-4960-8e5e-dd5b010d1898",
   "metadata": {},
   "source": [
    "#### Przygotowujemy się by zapisać model pod określoną ściężką oraz chcemy zapisać tylko najlepszy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b273a2a7-ba24-408d-8ad0-04bb7795eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = '/tmp/checkpoint.model.keras'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799007ec-19e8-441b-9ea8-f40cc13163da",
   "metadata": {},
   "source": [
    "#### Trenujemy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ceb4214-c672-4ffb-b01d-e0d8748ceb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m1772/1772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7737 - loss: 0.6439 - val_accuracy: 0.8460 - val_loss: 0.4146\n",
      "Epoch 2/12\n",
      "\u001b[1m1772/1772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8582 - loss: 0.3885 - val_accuracy: 0.8605 - val_loss: 0.3824\n",
      "Epoch 3/12\n",
      "\u001b[1m1772/1772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8729 - loss: 0.3442 - val_accuracy: 0.8783 - val_loss: 0.3395\n",
      "Epoch 4/12\n",
      "\u001b[1m1772/1772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8822 - loss: 0.3136 - val_accuracy: 0.8781 - val_loss: 0.3372\n",
      "Epoch 5/12\n",
      "\u001b[1m1772/1772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8874 - loss: 0.2995 - val_accuracy: 0.8725 - val_loss: 0.3530\n",
      "Epoch 6/12\n",
      "\u001b[1m1772/1772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8967 - loss: 0.2808 - val_accuracy: 0.8805 - val_loss: 0.3208\n",
      "Epoch 7/12\n",
      "\u001b[1m1772/1772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9002 - loss: 0.2655 - val_accuracy: 0.8887 - val_loss: 0.3078\n",
      "Epoch 8/12\n",
      "\u001b[1m1772/1772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9066 - loss: 0.2525 - val_accuracy: 0.8860 - val_loss: 0.3183\n",
      "Epoch 9/12\n",
      "\u001b[1m1772/1772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9071 - loss: 0.2468 - val_accuracy: 0.8908 - val_loss: 0.3151\n",
      "Epoch 10/12\n",
      "\u001b[1m1772/1772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9103 - loss: 0.2376 - val_accuracy: 0.8895 - val_loss: 0.3107\n",
      "Epoch 11/12\n",
      "\u001b[1m1772/1772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9163 - loss: 0.2272 - val_accuracy: 0.8910 - val_loss: 0.3144\n",
      "Epoch 12/12\n",
      "\u001b[1m1772/1772\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9168 - loss: 0.2201 - val_accuracy: 0.8921 - val_loss: 0.3085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c7de949b20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=12, batch_size=32,validation_split=0.1, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd8bc6-f671-4902-b79b-727ceb05440a",
   "metadata": {},
   "source": [
    "#### Wykorzystujemy najlepszy model do oceny na danych testowych "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c1320cb-82bc-4a2a-a40e-9371a32b8491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 746us/step - accuracy: 0.8929 - loss: 0.2966\n",
      "Test Accuracy: 0.8931428790092468\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5f208-b76f-4bbb-8181-07450114515d",
   "metadata": {},
   "source": [
    "#### Piszemy funkcję która dla najlepszego modelu wyświetli obraz zdefiniowanej wcześniej danej testowej i wyświetli przewidywaną labelkę. W pierszwym kroku zwiększamy ilość wymiarów elementu dla którego chcemy uzyskać przewidywaną labelkę, gdyż wyszkolony model potrzebuje jeszcze batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b685a5a2-ad09-436b-97cc-7064ee923160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_print(best_model, verified_image):\n",
    "    verified_image = np.expand_dims(verified_image, axis=0)\n",
    "    prediction = np.argmax(best_model.predict(verified_image), axis=1)[0]\n",
    "    plt.imshow(verified_image[0, ...,  0], cmap='grey')\n",
    "    plt.title(f\"Predicted Label: {prediction}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(\"0 \tT-shirt/top\\n1 \tTrouser\\n2 \tPullover\\n3 \tDress\\n4 \tCoat\\n5 \tSandal\")\n",
    "    print(\"6 \tShirt\\n7 \tSneaker\\n8 \tBags\\n9 \tAnkle boot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6b31d-c0f9-4db3-a99e-9a9ee9d5f4c7",
   "metadata": {},
   "source": [
    "#### Wywołanie funkcji z dowolną daną testową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a7fbb69-8b54-4303-9fb6-985d38d140c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tT-shirt/top\n",
      "1 \tTrouser\n",
      "2 \tPullover\n",
      "3 \tDress\n",
      "4 \tCoat\n",
      "5 \tSandal\n",
      "6 \tShirt\n",
      "7 \tSneaker\n",
      "8 \tBags\n",
      "9 \tAnkle boot\n"
     ]
    }
   ],
   "source": [
    "verified_image = X_test[612]\n",
    "predict_and_print(best_model,verified_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db9b6c-09c4-46fc-93e3-90494933b55a",
   "metadata": {},
   "source": [
    "#### Przechodzimy do zadania zwiększenia ilości próbek w zbiorze treningowym. Będziemy tworzyć obrazy z limitami kontrastu i jasności na poziomie 0.2. Następnie piszemy funkcję która bedzie generować obrazy i zapisywać je do listy. W drugiej liście będą zapisywane labelki obrazów na podstawie których są trzorzone zmodyfikowane obrazy. Funkcja zwraca obie listy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b85e40dc-ba3a-406b-86cc-cf55e754e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = A.Compose([\n",
    "    A.RandomBrightnessContrast( contrast_limit=0.2, brightness_limit=0.2, p=0.6),   # Zmiana jasności i kontrastu\n",
    "])\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "def augment_images(images, labels, augmenter):\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    images_number = np.random.randint(30000, 56000)\n",
    "    for i in tqdm(range(images_number)):\n",
    "        augmented = augmenter(image=images[i])['image']\n",
    "        augmented_images.append(augmented)\n",
    "        augmented_labels.append(labels[i])\n",
    "    return np.array(augmented_images), np.array(augmented_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c92d1-220b-4cea-9ecc-42b8bbb83be8",
   "metadata": {},
   "source": [
    "#### Wywołujemy naszą funkcję i listy zwrócone przez naszą funkcję zapisujemy pod dwoma zmiennymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ab62587-d1fc-4f70-bb48-49761ae7155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 44522/44522 [00:01<00:00, 38794.09it/s]\n"
     ]
    }
   ],
   "source": [
    "augmented_X_train, augmented_y_train = augment_images(X_train, y_train , augmentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21617ca8-5412-44b7-8d08-dd0c1a5f5e3d",
   "metadata": {},
   "source": [
    "#### Dodajemy Obrazy i etykiety które wygenerowaliśmy do pierwotnych danych treningowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5e22dcb-dac7-4238-88b7-ddb83f5523b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train, augmented_X_train), axis=0)\n",
    "y_train =  np.concatenate((y_train, augmented_y_train), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68632f65-bba1-4683-b5b1-5f4ccaae0507",
   "metadata": {},
   "source": [
    "#### Przemieszamy dane treningowe, by dane pierwotne nie były brane do analizy w pierszej kolejności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1da6085-def8-4e61-9d5b-9c631fbd8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2ed73-7e12-43b6-888a-73ad8e89c915",
   "metadata": {},
   "source": [
    "#### Uczymy zdefiniowany wcześniej model, jednak modyfikujemy ścieżkę zapisu naszego modelu. Takie same pozostałe parametry, tylko danych treningowych mamy teraz więcej "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75db4f04-8b06-45bd-a912-57b22e909861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m3025/3025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9101 - loss: 0.2443 - val_accuracy: 0.9066 - val_loss: 0.2458\n",
      "Epoch 2/12\n",
      "\u001b[1m3025/3025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9178 - loss: 0.2179 - val_accuracy: 0.9105 - val_loss: 0.2464\n",
      "Epoch 3/12\n",
      "\u001b[1m3025/3025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9218 - loss: 0.2084 - val_accuracy: 0.9159 - val_loss: 0.2311\n",
      "Epoch 4/12\n",
      "\u001b[1m3025/3025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9224 - loss: 0.2057 - val_accuracy: 0.9155 - val_loss: 0.2245\n",
      "Epoch 5/12\n",
      "\u001b[1m3025/3025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9301 - loss: 0.1856 - val_accuracy: 0.9170 - val_loss: 0.2190\n",
      "Epoch 6/12\n",
      "\u001b[1m3025/3025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9316 - loss: 0.1787 - val_accuracy: 0.9140 - val_loss: 0.2296\n",
      "Epoch 7/12\n",
      "\u001b[1m3025/3025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9334 - loss: 0.1740 - val_accuracy: 0.9195 - val_loss: 0.2185\n",
      "Epoch 8/12\n",
      "\u001b[1m3025/3025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9372 - loss: 0.1665 - val_accuracy: 0.9226 - val_loss: 0.2122\n",
      "Epoch 9/12\n",
      "\u001b[1m3025/3025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9386 - loss: 0.1625 - val_accuracy: 0.9216 - val_loss: 0.2184\n",
      "Epoch 10/12\n",
      "\u001b[1m3025/3025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9415 - loss: 0.1569 - val_accuracy: 0.9221 - val_loss: 0.2180\n",
      "Epoch 11/12\n",
      "\u001b[1m3025/3025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9441 - loss: 0.1497 - val_accuracy: 0.9275 - val_loss: 0.1985\n",
      "Epoch 12/12\n",
      "\u001b[1m3025/3025\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9446 - loss: 0.1453 - val_accuracy: 0.9283 - val_loss: 0.2028\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8985 - loss: 0.3407  \n",
      "Test Accuracy: 0.9004285931587219\n"
     ]
    }
   ],
   "source": [
    "checkpoint_filepath = '/tmp/checkpoint.model2.keras'\n",
    "\n",
    "model.fit(X_train, y_train, epochs=12, batch_size=32,validation_split=0.1, callbacks=[model_checkpoint_callback])\n",
    "best_model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b3664-bc17-4267-b923-15133ac65134",
   "metadata": {},
   "source": [
    "#### Dzięki agumentacji udało się nieznacznie zwieększyć accuracy naszego modelu. Wykonuje dokładniejszą predykcję dla tych samych danych testowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93847f58-1492-4e56-9d7f-86b54150d89c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
