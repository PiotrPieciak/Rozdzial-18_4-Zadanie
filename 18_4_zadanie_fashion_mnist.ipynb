{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fbcc788-4cfa-49a1-a4c5-ac79104d6186",
   "metadata": {},
   "source": [
    "# 18.4 Zadanie Fashion-Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ce7b7-4d4e-4bac-acab-79446c9bf48a",
   "metadata": {},
   "source": [
    "#### import niezbednych bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5764a78-e959-4771-891d-758fcf57762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939e445c-6f94-4cc5-b103-26ba1a647b75",
   "metadata": {},
   "source": [
    "#### dane które są zaimportowane są już rozdzielone na testowe i treningowe, by móc zastosować train_test_split(images, labels, test_size=0.1, random_state=10, stratify=labels) połączymy oba zestawy danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c4cd71-63f7-4b4b-8ffc-c4bc71e79c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
    "images_train, labels_train = train\n",
    "images_test, labels_test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd63ed-799e-492c-8e51-57b60aae06da",
   "metadata": {},
   "source": [
    "#### Skoro mamy wszytskie dane pod zmienną images, można zastosowac normalizację obrazów oraz stworzyć dodatkowy wymiar niezbedny do szkolenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47954e7b-f392-49de-a1ec-e7de1b9591ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.concatenate((images_train, images_test), axis=0)\n",
    "images = images/255.0\n",
    "images = images[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "250b9e94-6aed-478e-af62-ebaffd09a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "labels =  np.concatenate((labels_train, labels_test), axis=0)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56741b5f-ef79-466e-9afa-44092499847d",
   "metadata": {},
   "source": [
    "#### Rozdzielamy dane zgodnie z tereścią zadania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90198a10-15e1-4161-a313-634d176b1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.1, random_state=10, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aabe41-2df3-4b55-b92b-1f7839eacad5",
   "metadata": {},
   "source": [
    "#### Tworzymy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7dafe22-c6d2-4075-b877-a3a5635c6576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piotr\\anaconda3\\envs\\condaenv4\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),  \n",
    "    tf.keras.layers.Dense(128, activation='relu'),     \n",
    "    tf.keras.layers.Dense(64, activation='relu'),     \n",
    "    tf.keras.layers.Dense(10, activation='softmax')   \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9930c-196e-4992-be07-a217b82a6a97",
   "metadata": {},
   "source": [
    "#### Kompilujemy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b990c79-4a47-4b32-8537-127e2c09f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7a154-4d82-4960-8e5e-dd5b010d1898",
   "metadata": {},
   "source": [
    "#### Przygotowujemy się by zapisać model pod określoną ściężką oraz chcemy zapisać tylko najlepszy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b273a2a7-ba24-408d-8ad0-04bb7795eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = '/tmp/checkpoint.model.keras'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799007ec-19e8-441b-9ea8-f40cc13163da",
   "metadata": {},
   "source": [
    "#### Trenujemy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ceb4214-c672-4ffb-b01d-e0d8748ceb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7533 - loss: 0.6950 - val_accuracy: 0.8537 - val_loss: 0.4103\n",
      "Epoch 2/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8630 - loss: 0.3835 - val_accuracy: 0.8678 - val_loss: 0.3666\n",
      "Epoch 3/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8735 - loss: 0.3415 - val_accuracy: 0.8630 - val_loss: 0.3772\n",
      "Epoch 4/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8801 - loss: 0.3244 - val_accuracy: 0.8802 - val_loss: 0.3334\n",
      "Epoch 5/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8880 - loss: 0.2988 - val_accuracy: 0.8833 - val_loss: 0.3258\n",
      "Epoch 6/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8947 - loss: 0.2821 - val_accuracy: 0.8730 - val_loss: 0.3413\n",
      "Epoch 7/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8966 - loss: 0.2747 - val_accuracy: 0.8873 - val_loss: 0.3095\n",
      "Epoch 8/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9000 - loss: 0.2638 - val_accuracy: 0.8868 - val_loss: 0.3182\n",
      "Epoch 9/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9053 - loss: 0.2546 - val_accuracy: 0.8919 - val_loss: 0.3096\n",
      "Epoch 10/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9086 - loss: 0.2423 - val_accuracy: 0.8883 - val_loss: 0.3049\n",
      "Epoch 11/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9122 - loss: 0.2328 - val_accuracy: 0.8917 - val_loss: 0.3160\n",
      "Epoch 12/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9145 - loss: 0.2285 - val_accuracy: 0.8890 - val_loss: 0.3237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18ddcf7f800>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=12, batch_size=64,validation_split=0.1, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd8bc6-f671-4902-b79b-727ceb05440a",
   "metadata": {},
   "source": [
    "#### Wykorzystujemy najlepszy model do oceny na danych testowych "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c1320cb-82bc-4a2a-a40e-9371a32b8491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 902us/step - accuracy: 0.8886 - loss: 0.3055\n",
      "Test Accuracy: 0.8917142748832703\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5f208-b76f-4bbb-8181-07450114515d",
   "metadata": {},
   "source": [
    "#### Piszemy funkcję która dla najlepszego modelu wyświetli obraz zdefiniowanej wcześniej danej testowej i wyświetli przewidywaną labelkę. W pierszwym kroku zwiększamy ilość wymiarów elementu dla którego chcemy uzyskać przewidywaną labelkę, gdyż wyszkolony model potrzebuje jeszcze batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b685a5a2-ad09-436b-97cc-7064ee923160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_print(best_model, verified_image):\n",
    "    verified_image = np.expand_dims(verified_image, axis=0)\n",
    "    prediction = np.argmax(best_model.predict(verified_image), axis=1)[0]\n",
    "    plt.imshow(verified_image[0, ...,  0], cmap='grey')\n",
    "    plt.title(f\"Predicted Label: {prediction}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(\"0 \tT-shirt/top\\n1 \tTrouser\\n2 \tPullover\\n3 \tDress\\n4 \tCoat\\n5 \tSandal\")\n",
    "    print(\"6 \tShirt\\n7 \tSneaker\\n8 \tBags\\n9 \tAnkle boot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6b31d-c0f9-4db3-a99e-9a9ee9d5f4c7",
   "metadata": {},
   "source": [
    "#### Wywołanie funkcji z dowolną daną testową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a7fbb69-8b54-4303-9fb6-985d38d140c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tT-shirt/top\n",
      "1 \tTrouser\n",
      "2 \tPullover\n",
      "3 \tDress\n",
      "4 \tCoat\n",
      "5 \tSandal\n",
      "6 \tShirt\n",
      "7 \tSneaker\n",
      "8 \tBags\n",
      "9 \tAnkle boot\n"
     ]
    }
   ],
   "source": [
    "verified_image = X_test[26]\n",
    "predict_and_print(best_model,verified_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db9b6c-09c4-46fc-93e3-90494933b55a",
   "metadata": {},
   "source": [
    "#### Przechodzimy do zadania zwiększenia ilości próbek w zbiorze treningowym. Będziemy tworzyć obrazy z limitami kontrastu i jasności na poziomie 0.2. Następnie piszemy funkcję która bedzie generować obrazy i zapisywać je do listy. W drugiej liście będą zapisywane labelki obrazów na podstawie których są trzorzone zmodyfikowane obrazy. Funkcja zwraca obie listy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b85e40dc-ba3a-406b-86cc-cf55e754e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = A.Compose([\n",
    "    A.RandomBrightnessContrast( contrast_limit=0.2, brightness_limit=0.2, p=1)\n",
    "])\n",
    "\n",
    "def augment_images(images, labels, augmenter):\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    images_numbers = np.random.randint(int(len(X_train)*0.5), int(len(X_train)*0.7))\n",
    "    index_list = random.sample(np.arange(0, len(X_train), 1).tolist(), images_numbers)\n",
    "    for i in index_list:\n",
    "        augmented = augmenter(image=images[i])['image']\n",
    "        augmented_images.append(augmented)\n",
    "        augmented_labels.append(labels[i])\n",
    "    print(\"Stworzono\", len(index_list) ,\"dodatkowych zdjęć podczas agumentacji\")\n",
    "    return np.array(augmented_images), np.array(augmented_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c92d1-220b-4cea-9ecc-42b8bbb83be8",
   "metadata": {},
   "source": [
    "#### Wywołujemy naszą funkcję i listy zwrócone przez naszą funkcję zapisujemy pod dwoma zmiennymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ab62587-d1fc-4f70-bb48-49761ae7155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stworzono 33702 dodatkowych zdjęć podczas agumentacji\n"
     ]
    }
   ],
   "source": [
    "augmented_X_train, augmented_y_train = augment_images(X_train, y_train , augmentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21617ca8-5412-44b7-8d08-dd0c1a5f5e3d",
   "metadata": {},
   "source": [
    "#### Dodajemy Obrazy i etykiety które wygenerowaliśmy do pierwotnych danych treningowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5e22dcb-dac7-4238-88b7-ddb83f5523b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train, augmented_X_train), axis=0)\n",
    "y_train =  np.concatenate((y_train, augmented_y_train), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68632f65-bba1-4683-b5b1-5f4ccaae0507",
   "metadata": {},
   "source": [
    "#### Przemieszamy dane treningowe, by dane pierwotne nie były brane do analizy w pierszej kolejności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1da6085-def8-4e61-9d5b-9c631fbd8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2ed73-7e12-43b6-888a-73ad8e89c915",
   "metadata": {},
   "source": [
    "#### Uczymy zdefiniowany wcześniej model, jednak modyfikujemy ścieżkę zapisu naszego modelu. Takie same pozostałe parametry, tylko danych treningowych mamy teraz więcej "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75db4f04-8b06-45bd-a912-57b22e909861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9036 - loss: 0.2571 - val_accuracy: 0.8957 - val_loss: 0.2690\n",
      "Epoch 2/12\n",
      "\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9147 - loss: 0.2285 - val_accuracy: 0.9118 - val_loss: 0.2463\n",
      "Epoch 3/12\n",
      "\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9180 - loss: 0.2195 - val_accuracy: 0.9063 - val_loss: 0.2520\n",
      "Epoch 4/12\n",
      "\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9213 - loss: 0.2066 - val_accuracy: 0.9131 - val_loss: 0.2328\n",
      "Epoch 5/12\n",
      "\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9260 - loss: 0.1973 - val_accuracy: 0.9066 - val_loss: 0.2523\n",
      "Epoch 6/12\n",
      "\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9268 - loss: 0.1927 - val_accuracy: 0.9169 - val_loss: 0.2193\n",
      "Epoch 7/12\n",
      "\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9310 - loss: 0.1841 - val_accuracy: 0.9084 - val_loss: 0.2414\n",
      "Epoch 8/12\n",
      "\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9344 - loss: 0.1737 - val_accuracy: 0.9182 - val_loss: 0.2258\n",
      "Epoch 9/12\n",
      "\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9373 - loss: 0.1702 - val_accuracy: 0.9130 - val_loss: 0.2258\n",
      "Epoch 10/12\n",
      "\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9373 - loss: 0.1663 - val_accuracy: 0.9235 - val_loss: 0.2111\n",
      "Epoch 11/12\n",
      "\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9426 - loss: 0.1533 - val_accuracy: 0.9137 - val_loss: 0.2441\n",
      "Epoch 12/12\n",
      "\u001b[1m1360/1360\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9427 - loss: 0.1510 - val_accuracy: 0.9216 - val_loss: 0.2296\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - accuracy: 0.8998 - loss: 0.3039\n",
      "Test Accuracy: 0.8988571166992188\n"
     ]
    }
   ],
   "source": [
    "checkpoint_filepath = '/tmp/checkpoint.model2.keras'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=12, batch_size=64,validation_split=0.1, callbacks=[model_checkpoint_callback])\n",
    "best_model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b3664-bc17-4267-b923-15133ac65134",
   "metadata": {},
   "source": [
    "#### Dzięki agumentacji udało się nieznacznie zwieększyć accuracy naszego modelu. Wykonuje dokładniejszą predykcję dla tych samych danych testowych"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
