{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fbcc788-4cfa-49a1-a4c5-ac79104d6186",
   "metadata": {},
   "source": [
    "# 18.4 Zadanie Fashion-Mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ce7b7-4d4e-4bac-acab-79446c9bf48a",
   "metadata": {},
   "source": [
    "#### import niezbednych bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5764a78-e959-4771-891d-758fcf57762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939e445c-6f94-4cc5-b103-26ba1a647b75",
   "metadata": {},
   "source": [
    "#### dane które są zaimportowane są już rozdzielone na testowe i treningowe, by móc zastosować train_test_split(images, labels, test_size=0.1, random_state=10, stratify=labels) połączymy oba zestawy danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18c4cd71-63f7-4b4b-8ffc-c4bc71e79c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
    "images_train, labels_train = train\n",
    "images_test, labels_test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd63ed-799e-492c-8e51-57b60aae06da",
   "metadata": {},
   "source": [
    "#### Skoro mamy wszytskie dane pod zmienną images, można zastosowac normalizację obrazów oraz stworzyć dodatkowy wymiar niezbedny do szkolenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47954e7b-f392-49de-a1ec-e7de1b9591ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.concatenate((images_train, images_test), axis=0)\n",
    "images = images/255.0\n",
    "images = images[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "250b9e94-6aed-478e-af62-ebaffd09a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "labels =  np.concatenate((labels_train, labels_test), axis=0)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56741b5f-ef79-466e-9afa-44092499847d",
   "metadata": {},
   "source": [
    "#### Rozdzielamy dane zgodnie z tereścią zadania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90198a10-15e1-4161-a313-634d176b1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.1, random_state=10, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aabe41-2df3-4b55-b92b-1f7839eacad5",
   "metadata": {},
   "source": [
    "#### Tworzymy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7dafe22-c6d2-4075-b877-a3a5635c6576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\piotr\\anaconda3\\envs\\condaenv4\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),  \n",
    "    tf.keras.layers.Dense(128, activation='relu'),     \n",
    "    tf.keras.layers.Dense(64, activation='relu'),     \n",
    "    tf.keras.layers.Dense(10, activation='softmax')   \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9930c-196e-4992-be07-a217b82a6a97",
   "metadata": {},
   "source": [
    "#### Kompilujemy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b990c79-4a47-4b32-8537-127e2c09f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7a154-4d82-4960-8e5e-dd5b010d1898",
   "metadata": {},
   "source": [
    "#### Przygotowujemy się by zapisać model pod określoną ściężką oraz chcemy zapisać tylko najlepszy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b273a2a7-ba24-408d-8ad0-04bb7795eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = '/tmp/checkpoint.model.keras'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799007ec-19e8-441b-9ea8-f40cc13163da",
   "metadata": {},
   "source": [
    "#### Trenujemy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ceb4214-c672-4ffb-b01d-e0d8748ceb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7623 - loss: 0.6922 - val_accuracy: 0.8546 - val_loss: 0.4088\n",
      "Epoch 2/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8580 - loss: 0.3893 - val_accuracy: 0.8752 - val_loss: 0.3483\n",
      "Epoch 3/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8744 - loss: 0.3419 - val_accuracy: 0.8714 - val_loss: 0.3668\n",
      "Epoch 4/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8815 - loss: 0.3194 - val_accuracy: 0.8786 - val_loss: 0.3324\n",
      "Epoch 5/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8907 - loss: 0.2988 - val_accuracy: 0.8816 - val_loss: 0.3261\n",
      "Epoch 6/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8936 - loss: 0.2869 - val_accuracy: 0.8856 - val_loss: 0.3159\n",
      "Epoch 7/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9002 - loss: 0.2690 - val_accuracy: 0.8887 - val_loss: 0.3054\n",
      "Epoch 8/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8988 - loss: 0.2659 - val_accuracy: 0.8778 - val_loss: 0.3321\n",
      "Epoch 9/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9040 - loss: 0.2544 - val_accuracy: 0.8894 - val_loss: 0.3113\n",
      "Epoch 10/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9088 - loss: 0.2435 - val_accuracy: 0.8875 - val_loss: 0.3205\n",
      "Epoch 11/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9114 - loss: 0.2370 - val_accuracy: 0.8854 - val_loss: 0.3280\n",
      "Epoch 12/12\n",
      "\u001b[1m886/886\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9149 - loss: 0.2260 - val_accuracy: 0.8914 - val_loss: 0.3153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14300118a40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=12, batch_size=64,validation_split=0.1, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfd8bc6-f671-4902-b79b-727ceb05440a",
   "metadata": {},
   "source": [
    "#### Wykorzystujemy najlepszy model do oceny na danych testowych "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c1320cb-82bc-4a2a-a40e-9371a32b8491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8953 - loss: 0.2996\n",
      "Test Accuracy: 0.8987143039703369\n"
     ]
    }
   ],
   "source": [
    "best_model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce5f208-b76f-4bbb-8181-07450114515d",
   "metadata": {},
   "source": [
    "#### Piszemy funkcję która dla najlepszego modelu wyświetli obraz zdefiniowanej wcześniej danej testowej i wyświetli przewidywaną labelkę. W pierszwym kroku zwiększamy ilość wymiarów elementu dla którego chcemy uzyskać przewidywaną labelkę, gdyż wyszkolony model potrzebuje jeszcze batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b685a5a2-ad09-436b-97cc-7064ee923160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_print(best_model, verified_image):\n",
    "    verified_image = np.expand_dims(verified_image, axis=0)\n",
    "    prediction = np.argmax(best_model.predict(verified_image), axis=1)[0]\n",
    "    plt.imshow(verified_image[0, ...,  0], cmap='grey')\n",
    "    plt.title(f\"Predicted Label: {prediction}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(\"0 \tT-shirt/top\\n1 \tTrouser\\n2 \tPullover\\n3 \tDress\\n4 \tCoat\\n5 \tSandal\")\n",
    "    print(\"6 \tShirt\\n7 \tSneaker\\n8 \tBags\\n9 \tAnkle boot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b6b31d-c0f9-4db3-a99e-9a9ee9d5f4c7",
   "metadata": {},
   "source": [
    "#### Wywołanie funkcji z dowolną daną testową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a7fbb69-8b54-4303-9fb6-985d38d140c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tT-shirt/top\n",
      "1 \tTrouser\n",
      "2 \tPullover\n",
      "3 \tDress\n",
      "4 \tCoat\n",
      "5 \tSandal\n",
      "6 \tShirt\n",
      "7 \tSneaker\n",
      "8 \tBags\n",
      "9 \tAnkle boot\n"
     ]
    }
   ],
   "source": [
    "verified_image = X_test[0]\n",
    "predict_and_print(best_model,verified_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3db9b6c-09c4-46fc-93e3-90494933b55a",
   "metadata": {},
   "source": [
    "#### Przechodzimy do zadania zwiększenia ilości próbek w zbiorze treningowym. Będziemy tworzyć obrazy z limitami kontrastu i jasności na poziomie 0.2. Następnie piszemy funkcję która bedzie generować obrazy i zapisywać je do listy. W drugiej liście będą zapisywane labelki obrazów na podstawie których są trzorzone zmodyfikowane obrazy. Funkcja zwraca obie listy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b85e40dc-ba3a-406b-86cc-cf55e754e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = A.Compose([\n",
    "    A.RandomBrightnessContrast( contrast_limit=0.2, brightness_limit=0.2, p=0.6),# Zmiana jasności i kontrast\n",
    "])\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "def augment_images(images, labels, augmenter):\n",
    "    augmented_images = []\n",
    "    augmented_labels = []\n",
    "    images_numbers = np.random.randint(int(len(X_train)*0.5), int(len(X_train)*0.7))\n",
    "    for i in tqdm(range(images_numbers)):\n",
    "        random_image_idx = np.random.randint(0, len(X_train))\n",
    "        augmented = augmenter(image=images[random_image_idx])['image']\n",
    "        augmented_images.append(augmented)\n",
    "        augmented_labels.append(labels[random_image_idx])\n",
    "    return np.array(augmented_images), np.array(augmented_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c92d1-220b-4cea-9ecc-42b8bbb83be8",
   "metadata": {},
   "source": [
    "#### Wywołujemy naszą funkcję i listy zwrócone przez naszą funkcję zapisujemy pod dwoma zmiennymi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ab62587-d1fc-4f70-bb48-49761ae7155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 33338/33338 [00:01<00:00, 26843.93it/s]\n"
     ]
    }
   ],
   "source": [
    "augmented_X_train, augmented_y_train = augment_images(X_train, y_train , augmentations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21617ca8-5412-44b7-8d08-dd0c1a5f5e3d",
   "metadata": {},
   "source": [
    "#### Dodajemy Obrazy i etykiety które wygenerowaliśmy do pierwotnych danych treningowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5e22dcb-dac7-4238-88b7-ddb83f5523b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train, augmented_X_train), axis=0)\n",
    "y_train =  np.concatenate((y_train, augmented_y_train), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68632f65-bba1-4683-b5b1-5f4ccaae0507",
   "metadata": {},
   "source": [
    "#### Przemieszamy dane treningowe, by dane pierwotne nie były brane do analizy w pierszej kolejności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1da6085-def8-4e61-9d5b-9c631fbd8086",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2ed73-7e12-43b6-888a-73ad8e89c915",
   "metadata": {},
   "source": [
    "#### Uczymy zdefiniowany wcześniej model, jednak modyfikujemy ścieżkę zapisu naszego modelu. Takie same pozostałe parametry, tylko danych treningowych mamy teraz więcej "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75db4f04-8b06-45bd-a912-57b22e909861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9078 - loss: 0.2459 - val_accuracy: 0.9103 - val_loss: 0.2433\n",
      "Epoch 2/12\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9152 - loss: 0.2237 - val_accuracy: 0.9071 - val_loss: 0.2487\n",
      "Epoch 3/12\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9218 - loss: 0.2084 - val_accuracy: 0.9193 - val_loss: 0.2121\n",
      "Epoch 4/12\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9256 - loss: 0.1983 - val_accuracy: 0.9110 - val_loss: 0.2379\n",
      "Epoch 5/12\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9295 - loss: 0.1897 - val_accuracy: 0.9181 - val_loss: 0.2201\n",
      "Epoch 6/12\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9320 - loss: 0.1820 - val_accuracy: 0.9151 - val_loss: 0.2294\n",
      "Epoch 7/12\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9347 - loss: 0.1740 - val_accuracy: 0.9199 - val_loss: 0.2188\n",
      "Epoch 8/12\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9370 - loss: 0.1659 - val_accuracy: 0.9209 - val_loss: 0.2199\n",
      "Epoch 9/12\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9382 - loss: 0.1629 - val_accuracy: 0.9163 - val_loss: 0.2234\n",
      "Epoch 10/12\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9432 - loss: 0.1557 - val_accuracy: 0.9241 - val_loss: 0.2189\n",
      "Epoch 11/12\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9425 - loss: 0.1510 - val_accuracy: 0.9216 - val_loss: 0.2151\n",
      "Epoch 12/12\n",
      "\u001b[1m1355/1355\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9463 - loss: 0.1455 - val_accuracy: 0.9204 - val_loss: 0.2360\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 854us/step - accuracy: 0.9004 - loss: 0.3462\n",
      "Test Accuracy: 0.9022856950759888\n"
     ]
    }
   ],
   "source": [
    "checkpoint_filepath = '/tmp/checkpoint.model2.keras'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=12, batch_size=64,validation_split=0.1, callbacks=[model_checkpoint_callback])\n",
    "best_model = tf.keras.models.load_model(checkpoint_filepath)\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b3664-bc17-4267-b923-15133ac65134",
   "metadata": {},
   "source": [
    "#### Dzięki agumentacji udało się nieznacznie zwieększyć accuracy naszego modelu. Wykonuje dokładniejszą predykcję dla tych samych danych testowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbf4b4d-512f-427e-be3c-f35965e7e397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
